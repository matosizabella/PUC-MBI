{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carregamento das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as scs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploração dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este capítulo do Notebook apresenta como foram tratados os dados para posterior análise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Conhecendo o DF, prévia e tipo dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conhecendo o DF\n",
    "df= pd.read_csv('dados_finais.csv')\n",
    "\n",
    "print('shape:', df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conhecendo uma prévia dos dados\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setando o ID como index do DF\n",
    "df.set_index('ID', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checando os nomes das colunas\n",
    "df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checando os tipos dos dados\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Verificação de dados duplicados, checagens e tratamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando se há dados de index duplicados\n",
    "df[df.index.duplicated()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validando se o número total de homens+mulheres bate com o total da população\n",
    "df[(df['HOMEMTOT']+df['MULHERTOT']) !=df['TPM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando se as colunas da raspagem de dados há dados duplicados:\n",
    "colunas=['TPM', 'DDM', 'SMM',\n",
    "       'POM', 'TPO', 'RSA', 'TXE', 'IDEBI', 'IDEBF', 'PIB', 'RFE', 'RER',\n",
    "       'DEM', 'OPNV', 'IMH', 'AUT', 'ESA', 'AVP', 'UVP']\n",
    "df[df.duplicated(colunas)==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não foram verificados dados duplicados e/ou discrepantes entre as bases pesquisadas (Atlas Brasil e IBGE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Verificação de dados faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando dados faltantes (por atributo)\n",
    "nulos=df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(nulos[nulos>0].index,nulos[nulos>0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nulos[nulos>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total de registros faltantes no dataset\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alguns atributos apresentaram altas taxas de dados faltantes, optou-se por exclui-los do dataset ao invés de preenchê-los com a média ou moda para não criar vieses. Sendo assim, os atributos \"OPNV\", \"RFE\" e \"IMH\" foram retirados do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original=df.copy()\n",
    "\n",
    "df.drop(columns=['OPNV','RFE','IMH'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para os demais dados faltantes, o procedimento realizado foi o preenchimento de acordo com a média dos valores de cada atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preenchendo os dados faltantes com a média dos valores:\n",
    "df.fillna(df.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total de dados faltantes após o tratamento:\", df.isna().sum().sum())\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descrição da coluna de Score (GGI):\n",
    "df['SCO'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Padronização dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com um dataset composto por diversos atributos de diferentes grandezas, outro tratamento necessário é a padronização dos dados. A padronização foi realizada em uma parte do dataset, que desconsidera os atributos categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Separando os valores no Dataset\n",
    "p = df.iloc[:,4:]\n",
    "colunas=p.columns\n",
    "\n",
    "\n",
    "# padronizando os atributos\n",
    "p = StandardScaler().fit_transform(p)\n",
    "\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p=pd.DataFrame(p, columns=colunas, index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Aplicação dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste capítulo são apresentadas as aplicações dos algoritmos, métricas e otimizações utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Otimização de hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_cluster(algorithm_name, X, upK,downK=1):\n",
    "    list_n_k=[]\n",
    "    list_silh=[]\n",
    "    \n",
    "    silh_max=-1e12\n",
    "    k_silh_max=0\n",
    "    \n",
    "    if algorithm_name=='KMeans':\n",
    "        algorithm=cluster.KMeans(n_clusters=1)\n",
    "        \n",
    "    elif algorithm_name=='Ward':\n",
    "        algorithm=cluster.AgglomerativeClustering(n_clusters=1, linkage='ward')\n",
    "        \n",
    "    else: \n",
    "        print('Algoritmo inválido')\n",
    "        return 0\n",
    "    \n",
    "    for iterator in range(downK,upK+1):\n",
    "        \n",
    "        if iterator==1:\n",
    "            list_n_k.append(1)\n",
    "            list_silh.append(0)\n",
    "            continue\n",
    "        \n",
    "        algorithm.n_clusters=iterator \n",
    "        \n",
    "        cluster_labels=algorithm.fit_predict(X)\n",
    "        \n",
    "        list_n_k.append(iterator)\n",
    "        silh=metrics.silhouette_score(X,cluster_labels)\n",
    "        list_silh.append(silh)\n",
    "        \n",
    "        if silh>silh_max:\n",
    "            silh_max= silh\n",
    "            k_silh_max=iterator\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "        \n",
    "    plt.title(\"Busca do silhouette ótimo - \"+algorithm_name)\n",
    "    plt.grid()\n",
    "        \n",
    "    plt.plot(list_n_k, list_silh) \n",
    "        \n",
    "    return print(algorithm_name,\" - Número de clusters ideais considerando a métrica silhouette: \", k_silh_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Número de Clusters: K-Means "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a métrica Silhouette para avaliação do número ideal de clusters no algoritmo KMeans, temos que o melhor valor encontrado sugere um k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cluster(\"KMeans\",p, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra forma de avaliação para o número de clusters é a avaliação através do \"Método Elbow\", que avalia o somatório dos erros quadráticos das instâncias de cada cluster. Neste método, podemos observar que o número de clusters ideais também sugere k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    " \n",
    "for i in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters = i, random_state=10)\n",
    "    kmeans.fit(p)\n",
    "    print (i,kmeans.inertia_)\n",
    "    wcss.append(kmeans.inertia_)  \n",
    "plt.plot(range(1, 15), wcss)\n",
    "plt.title('O Metodo Elbow')\n",
    "plt.grid()\n",
    "plt.xlabel('Numero de Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 - Número de Clusters: Clustering Hierárquico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra abordagem é a utilização de algoritmos de clustering hierárquico. Neste trabalho utilizou-se a abordagem aglomerativa com o critério \"Ward\", que minimiza a variância dos clusters mesclados. \n",
    "Ao aplicarmos a otimização de hiperparâmetros de acordo com a métrica silhouette, verificou-se que o número de clusters ótimo é 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cluster(\"Ward\",p, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Aplicação do K-Means para dados padronizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as avaliações no item 3.1.1, foi identificado que o número de clusters ideais para aplicação do KMeans neste dataset é 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, random_state=10)\n",
    "\n",
    "kmeans.fit(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = kmeans.fit_transform(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "pl=labels\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando o resultado do algoritmo, temos que somente duas instâncias foram agrupadas no cluster 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans=df.copy()\n",
    "df_kmeans['Cluster']=pd.Series(pl).values\n",
    "df_kmeans['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As duas instâncias que foram agrupadas no cluster 2 foram Rio de Janeiro e São Paulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans[df_kmeans['Cluster']==2][['UF','NOMEMUN','Cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar tambem que regiões Norte e Nordeste possuem a maioria dos seus municípios classificados como Cluster 0, enquanto que nas demais regiões, a maioria dos municipios foi classificada como Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans[['REGIAO','Cluster','NOMEMUN']].groupby(['REGIAO','Cluster']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans.groupby(['REGIAO','UF','Cluster']).size().unstack().plot(kind='bar',stacked=True)\n",
    "plt.title('Quantidade de cidades por cluster em cada estado')\n",
    "plt.ylabel('Quantidade de cidades')\n",
    "plt.xlabel('Estados brasileiros')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicação do PCA para redução de dimensionalidade e visualização dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=2)\n",
    "P=pca.fit_transform(p)\n",
    "\n",
    "print('Soma da variância acumulada:{}'.format(pca.explained_variance_ratio_ .sum()))\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a redução da dimensionalidade para n=2, temos uma variância acumulada de aproximadamente 60% dos dados. Para uma representação melhor, precisariamos de um n maior mas não seria interessante para observação gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_k_pca=pd.DataFrame(P, columns=['PC1', 'PC2'])\n",
    "df_k_pca['Cluster']=pl\n",
    "df_k_pca['Cidade']=df_kmeans['NOMEMUN'].values\n",
    "df_k_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, metrics\n",
    "\n",
    "kmeans.fit(P)\n",
    "\n",
    "kmeans.cluster_medoids_=[]\n",
    "\n",
    "medians_index, _ = metrics.pairwise_distances_argmin_min(kmeans.cluster_centers_ , P[:,:2])\n",
    "\n",
    "medians_index #array com os indices das instancias dos clusters\n",
    "\n",
    "for m in medians_index:\n",
    "    kmeans.cluster_medoids_.append(np.array( [P[m,0],P[m,1]] ))\n",
    "\n",
    "kmeans.cluster_medoids_\n",
    "\n",
    "labels=kmeans.labels_\n",
    "centers=kmeans.cluster_centers_\n",
    "medoids=np.array(kmeans.cluster_medoids_)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "\n",
    "plt.scatter(P[:,0],P[:,1], c=labels, label=\"Instâncias\")\n",
    "\n",
    "\n",
    "plt.scatter(centers[:,0],centers[:,1], marker='X', c=np.unique(labels),s=200, ec='k', alpha=0.6, label='Centróide')\n",
    "plt.scatter(medoids[:,0],medoids[:,1], marker='o', c=\"None\",s=100, ec='k', alpha=0.6, label=\"Medóide\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificando os medóides de cada cluster, temos as cidades de AQUIDABÃ (Sergipe), Conceição das Alagoas (MG) e São Paulo (SP), como instâncias mais próximas dos clusters 0, 1, 2 respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_k_pca[df_k_pca['PC1'].isin(medoids[:,0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, podemos verificar os principais atributos em cada Componente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={key:[x,y] for key,x,y in zip(colunas,pca.components_[0],pca.components_[1])}\n",
    "\n",
    "rank_pc1 = [(colunas, pca_components_[0]) for colunas, pca_components_ in sorted(a.items(), \n",
    "                                                                                 key=lambda x: abs(x[1][0]), reverse=True)]\n",
    "\n",
    "rank_pc2 = [(colunas, pca_components_[1]) for colunas, pca_components_ in sorted(a.items(), \n",
    "                                                                                 key=lambda x: abs(x[1][1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(\"PC1 - Atributo\",i+1,\":\",rank_pc1 [:4][i])\n",
    "\n",
    "print('-'*60)\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"PC2 - Atributo\",i+1,\":\",rank_pc2 [:4][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. - Aplicação de Clustering Hierárquico em dados padronizados: Ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "\n",
    "w.fit_predict(p)\n",
    "\n",
    "labels_ward=w.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ward=df_p.copy()\n",
    "df_ward['Cluster']=pd.Series(labels_ward).values\n",
    "df_ward['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ward['Cidade']=df['NOMEMUN']\n",
    "df_ward['Cidade'][df_ward['Cluster']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ward_pca=pd.DataFrame(P, columns=['PC1', 'PC2'])\n",
    "df_ward_pca['Cluster']=df_ward['Cluster'].values\n",
    "df_ward_pca['Cidade']=df_ward['Cidade'].values\n",
    "df_ward_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=df_ward_pca['PC1'], y=df_ward_pca['PC2'], hue=df_ward_pca['Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ward['REGIAO']=df['REGIAO']\n",
    "df_ward['UF']=df['UF']\n",
    "df_ward.groupby(['REGIAO','UF','Cluster']).size().unstack().plot(kind='bar',stacked=True)\n",
    "plt.title('Quantidade de cidades por cluster em cada estado - WARD')\n",
    "plt.ylabel('Quantidade de cidades')\n",
    "plt.xlabel('Estados brasileiros')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
